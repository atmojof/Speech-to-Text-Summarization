{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = [\n",
    "        \"At Microsoft, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic, \"\n",
    "        \"human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Cognitive \"\n",
    "        \"Services, I have been working with a team of amazing scientists and engineers to turn this quest into a \"\n",
    "        \"reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of \"\n",
    "        \"human cognition: monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the \"\n",
    "        \"intersection of all three, there's magic-what we call XYZ-code as illustrated in Figure 1-a joint \"\n",
    "        \"representation to create more powerful AI that can speak, hear, see, and understand humans better. \"\n",
    "        \"We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, \"\n",
    "        \"spanning modalities and languages. The goal is to have pretrained models that can jointly learn \"\n",
    "        \"representations to support a broad range of downstream AI tasks, much in the way humans do today. \"\n",
    "        \"Over the past five years, we have achieved human performance on benchmarks in conversational speech \"\n",
    "        \"recognition, machine translation, conversational question answering, machine reading comprehension, \"\n",
    "        \"and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious \"\n",
    "        \"aspiration to produce a leap in AI capabilities, achieving multisensory and multilingual learning that \"\n",
    "        \"is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational \"\n",
    "        \"component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.getenv('AZURE_ENDPOINT')\n",
    "key = os.getenv(\"AZURE_KEY\")\n",
    "text_analytics_client = TextAnalyticsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(key),\n",
    ")\n",
    "\n",
    "#poller = text_analytics_client.begin_abstract_summary(document)\n",
    "abstract_summary_results = text_analytics_client.begin_abstract_summary(document).result()\n",
    "extract_summary_results = text_analytics_client.begin_extract_summary(document).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries abstracted:\n",
      "The Chief Technology Officer of Azure AI Cognitive Services discusses Microsoft's commitment to advancing AI by integrating monolingual text, audio/visual sensory signals, and multilingual data, termed the XYZ-code, to create AI that better understands humans across different domains. This innovative approach aims to enable cross-domain transfer learning, spanning modalities and languages, drawing inspiration from human cognition. The team's efforts over the past five years have led to breakthroughs in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning, achieving human-level performance on benchmarks. These advancements are seen as stepping stones toward a significant leap in AI capabilities, aspiring to develop multisensory and multilingual learning systems that mirror human learning processes. The ultimate goal is to ground the XYZ-code with external knowledge sources to enhance downstream AI tasks. This holistic strategy represents a pivotal shift in how Microsoft envisions the future of AI understanding and interaction.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in abstract_summary_results:\n",
    "    if result.kind == \"AbstractiveSummarization\":\n",
    "        print(\"Summaries abstracted:\")\n",
    "        [print(f\"{summary.text}\\n\") for summary in result.summaries]\n",
    "    elif result.is_error is True:\n",
    "        print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "            result.error.code, result.error.message\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary extracted: \n",
      "At the intersection of all three, there's magic-what we call XYZ-code as illustrated in Figure 1-a joint representation to create more powerful AI that can speak, hear, see, and understand humans better. We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages. The goal is to have pretrained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today.\n"
     ]
    }
   ],
   "source": [
    "for result in extract_summary_results:\n",
    "    if result.kind == \"ExtractiveSummarization\":\n",
    "        print(\"Summary extracted: \\n{}\".format(\n",
    "            \" \".join([sentence.text for sentence in result.sentences]))\n",
    "        )\n",
    "    elif result.is_error is True:\n",
    "        print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "            result.error.code, result.error.message\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SummarySentence(text=At the intersection of all three, there's magic-what we call XYZ-code as illustrated in Figure 1-a joint representation to create more powerful AI that can speak, hear, see, and understand humans better., rank_score=1.0, offset=517, length=203),\n",
       " SummarySentence(text=We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages., rank_score=0.86, offset=721, length=134),\n",
       " SummarySentence(text=The goal is to have pretrained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today., rank_score=0.96, offset=856, length=158)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now make a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(key),\n",
    "    )\n",
    "\n",
    "def azure_abstractive_summarization(text_input: str) -> str:\n",
    "    if not isinstance(text_input, list):\n",
    "        text_input = [text_input]\n",
    "    abstract_summary_results = text_analytics_client.begin_abstract_summary(text_input).result()\n",
    "    summaries_text = \"\"\n",
    "    for result in abstract_summary_results:\n",
    "        if result.kind == \"AbstractiveSummarization\":\n",
    "            summaries_text += \"\\n\".join([summary.text for summary in result.summaries]) + \" \"\n",
    "        elif result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                result.error.code, result.error.message\n",
    "            ))\n",
    "            return \"\"\n",
    "    return summaries_text\n",
    "\n",
    "def azure_extractive_summarization(text_input: str) -> str:\n",
    "    if not isinstance(text_input, list):\n",
    "        text_input = [text_input]\n",
    "    extract_summary_results = text_analytics_client.begin_extract_summary(text_input).result()\n",
    "    summaries_text = \"\"\n",
    "    for result in extract_summary_results:\n",
    "        if not result.is_error:\n",
    "            summaries_text += \"\\n\".join([sentence.text for sentence in result.sentences]) + \" \"\n",
    "        else:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                result.error.code, result.error.message\n",
    "            ))\n",
    "            return \"\"\n",
    "    return summaries_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_abstract = azure_abstractive_summarization(document)\n",
    "text_extract = azure_extractive_summarization(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Chief Technology Officer of Azure AI Cognitive Services discusses Microsoft's commitment to advancing AI by integrating monolingual text, audio/visual sensory signals, and multilingual data, termed the XYZ-code, to create AI that better understands humans across different domains. This innovative approach aims to enable cross-domain transfer learning, spanning modalities and languages, drawing inspiration from human cognition. The team's efforts over the past five years have led to breakthroughs in conversational speech recognition, machine translation, conversational question answering, machine reading comprehension, and image captioning, achieving human-level performance on benchmarks. These advancements are seen as stepping stones toward a significant leap in AI capabilities, aspiring to develop multisensory and multilingual learning systems that mirror human learning processes. The ultimate goal is to ground the XYZ-code with external knowledge sources to enhance downstream AI tasks. This holistic strategy represents a pivotal shift in how Microsoft envisions the future of AI understanding and interaction. \n",
      "At the intersection of all three, there's magic-what we call XYZ-code as illustrated in Figure 1-a joint representation to create more powerful AI that can speak, hear, see, and understand humans better.\n",
      "We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, spanning modalities and languages.\n",
      "The goal is to have pretrained models that can jointly learn representations to support a broad range of downstream AI tasks, much in the way humans do today. \n"
     ]
    }
   ],
   "source": [
    "print(text_abstract)\n",
    "print(text_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 247, Character count: 1627\n",
      "Word count: 150, Character count: 1132\n",
      "Word count: 78, Character count: 498\n"
     ]
    }
   ],
   "source": [
    "def count_words(text: str) -> tuple:\n",
    "    words = text.split()\n",
    "    word_count = len(words)\n",
    "    character_count = len(text)\n",
    "    \n",
    "    print(f\"Word count: {word_count}, Character count: {character_count}\")\n",
    "\n",
    "document_word_count = count_words(document[0])\n",
    "abstract_word_count = count_words(text_abstract)\n",
    "extract_word_count = count_words(text_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using own text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "(InvalidParameterValue) Job task: 'AbstractiveSummarization-0' failed with validation errors: ['Invalid Document in request.']\nCode: InvalidParameterValue\nMessage: Job task: 'AbstractiveSummarization-0' failed with validation errors: ['Invalid Document in request.']\nException Details:\t{'code': 'InvalidRequest', 'message': \"Job task: 'AbstractiveSummarization-0' failed with validation error: Request Payload sent is too large to be processed. Limit request size to: 125000\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\ai\\textanalytics\\_text_analytics_client.py:1250\u001b[0m, in \u001b[0;36mTextAnalyticsClient.begin_analyze_actions\u001b[1;34m(self, documents, actions, continuation_token, display_name, language, polling_interval, show_stats, **kwargs)\u001b[0m\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_language_api(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_version):\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m   1249\u001b[0m         AnalyzeActionsResponse,\n\u001b[1;32m-> 1250\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_analyze_text_submit_job\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAnalyzeTextJobsInput\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m                \u001b[49m\u001b[43manalysis_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerated_tasks\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpolling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAnalyzeActionsLROPollingMethod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtext_analytics_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1259\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolling_interval_arg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshow_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdoc_id_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_id_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtask_id_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlro_algorithms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mTextAnalyticsOperationResourcePolling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mshow_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontinuation_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinuation_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1273\u001b[0m     )\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;66;03m# v3.1\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\ai\\textanalytics\\_generated\\_operations_mixin.py:285\u001b[0m, in \u001b[0;36mTextAnalyticsClientOperationsMixin.begin_analyze_text_submit_job\u001b[1;34m(self, body, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m mixin_instance\u001b[38;5;241m.\u001b[39m_deserialize \u001b[38;5;241m=\u001b[39m Deserializer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_models_dict(api_version))\n\u001b[1;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmixin_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_analyze_text_submit_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:105\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\ai\\textanalytics\\_generated\\v2023_04_01\\operations\\_patch.py:67\u001b[0m, in \u001b[0;36mTextAnalyticsClientOperationsMixin.begin_analyze_text_submit_job\u001b[1;34m(self, body, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cont_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     raw_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_analyze_text_submit_job_initial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mz\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_map\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\ai\\textanalytics\\_generated\\v2023_04_01\\operations\\_text_analytics_client_operations.py:364\u001b[0m, in \u001b[0;36mTextAnalyticsClientOperationsMixin._analyze_text_submit_job_initial\u001b[1;34m(self, body, **kwargs)\u001b[0m\n\u001b[0;32m    363\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response)\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror)\n\u001b[0;32m    366\u001b[0m deserialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mHttpResponseError\u001b[0m: (InvalidRequest) Invalid Request.\nCode: InvalidRequest\nMessage: Invalid Request.\nException Details:\t(InvalidRequest) Job task: 'AbstractiveSummarization-0' failed with validation error: Request Payload sent is too large to be processed. Limit request size to: 125000\n\tCode: InvalidRequest\n\tMessage: Job task: 'AbstractiveSummarization-0' failed with validation error: Request Payload sent is too large to be processed. Limit request size to: 125000\nInner error: {\n    \"code\": \"InvalidParameterValue\",\n    \"message\": \"Job task: 'AbstractiveSummarization-0' failed with validation errors: ['Invalid Document in request.']\"\n}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\ai\\textanalytics\\_text_analytics_client.py:1926\u001b[0m, in \u001b[0;36mTextAnalyticsClient.begin_abstract_summary\u001b[1;34m(self, documents, continuation_token, disable_service_logs, display_name, language, polling_interval, show_stats, model_version, string_index_type, sentence_count, **kwargs)\u001b[0m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m   1923\u001b[0m         TextAnalysisLROPoller[\n\u001b[0;32m   1924\u001b[0m             ItemPaged[Union[AbstractiveSummaryResult, DocumentError]]\n\u001b[0;32m   1925\u001b[0m         ],\n\u001b[1;32m-> 1926\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_analyze_actions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m   1929\u001b[0m \u001b[43m                \u001b[49m\u001b[43mAbstractiveSummaryAction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1930\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodel_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1931\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstring_index_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstring_index_type_arg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1932\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msentence_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentence_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1933\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdisable_service_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_service_logs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1934\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1935\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1936\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpolling_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolling_interval_arg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1937\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1938\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshow_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbespoke\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1943\u001b[0m     )\n\u001b[0;32m   1945\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HttpResponseError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:105\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\ai\\textanalytics\\_validate.py:79\u001b[0m, in \u001b[0;36mvalidate_multiapi_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_api_version \u001b[38;5;241m==\u001b[39m VERSIONS_SUPPORTED[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version_method_added \u001b[38;5;129;01mand\u001b[39;00m version_method_added \u001b[38;5;241m!=\u001b[39m selected_api_version \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m     82\u001b[0m         VERSIONS_SUPPORTED\u001b[38;5;241m.\u001b[39mindex(selected_api_version) \u001b[38;5;241m<\u001b[39m VERSIONS_SUPPORTED\u001b[38;5;241m.\u001b[39mindex(version_method_added):\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\ai\\textanalytics\\_text_analytics_client.py:1324\u001b[0m, in \u001b[0;36mTextAnalyticsClient.begin_analyze_actions\u001b[1;34m(self, documents, actions, continuation_token, display_name, language, polling_interval, show_stats, **kwargs)\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HttpResponseError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m-> 1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess_http_response_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\ai\\textanalytics\\_response_handlers.py:63\u001b[0m, in \u001b[0;36mprocess_http_response_error\u001b[1;34m(error)\u001b[0m\n\u001b[0;32m     62\u001b[0m     raise_error \u001b[38;5;241m=\u001b[39m ResourceNotFoundError\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m raise_error(response\u001b[38;5;241m=\u001b[39merror\u001b[38;5;241m.\u001b[39mresponse, error_format\u001b[38;5;241m=\u001b[39mCSODataV4Format) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m\n",
      "\u001b[1;31mHttpResponseError\u001b[0m: (InvalidParameterValue) Job task: 'AbstractiveSummarization-0' failed with validation errors: ['Invalid Document in request.']\nCode: InvalidParameterValue\nMessage: Job task: 'AbstractiveSummarization-0' failed with validation errors: ['Invalid Document in request.']\nException Details:\t{'code': 'InvalidRequest', 'message': \"Job task: 'AbstractiveSummarization-0' failed with validation error: Request Payload sent is too large to be processed. Limit request size to: 125000\"}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./transcription.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      2\u001b[0m     transcription \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m----> 4\u001b[0m res_abstract \u001b[38;5;241m=\u001b[39m \u001b[43mazure_abstractive_summarization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscription\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./res_abstract.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      7\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(res_abstract)\n",
      "Cell \u001b[1;32mIn[65], line 9\u001b[0m, in \u001b[0;36mazure_abstractive_summarization\u001b[1;34m(text_input)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_input, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m      8\u001b[0m     text_input \u001b[38;5;241m=\u001b[39m [text_input]\n\u001b[1;32m----> 9\u001b[0m abstract_summary_results \u001b[38;5;241m=\u001b[39m \u001b[43mtext_analytics_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_abstract_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_input\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m     10\u001b[0m summaries_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m abstract_summary_results:\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:105\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\ai\\textanalytics\\_validate.py:79\u001b[0m, in \u001b[0;36mvalidate_multiapi_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# the latest version is selected, we assume all features supported\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_api_version \u001b[38;5;241m==\u001b[39m VERSIONS_SUPPORTED[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version_method_added \u001b[38;5;129;01mand\u001b[39;00m version_method_added \u001b[38;5;241m!=\u001b[39m selected_api_version \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m     82\u001b[0m         VERSIONS_SUPPORTED\u001b[38;5;241m.\u001b[39mindex(selected_api_version) \u001b[38;5;241m<\u001b[39m VERSIONS_SUPPORTED\u001b[38;5;241m.\u001b[39mindex(version_method_added):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not available in API version \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_api_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Use service API version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion_method_added\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or newer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\ai\\textanalytics\\_text_analytics_client.py:1946\u001b[0m, in \u001b[0;36mTextAnalyticsClient.begin_abstract_summary\u001b[1;34m(self, documents, continuation_token, disable_service_logs, display_name, language, polling_interval, show_stats, model_version, string_index_type, sentence_count, **kwargs)\u001b[0m\n\u001b[0;32m   1922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m   1923\u001b[0m         TextAnalysisLROPoller[\n\u001b[0;32m   1924\u001b[0m             ItemPaged[Union[AbstractiveSummaryResult, DocumentError]]\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1942\u001b[0m         )\n\u001b[0;32m   1943\u001b[0m     )\n\u001b[0;32m   1945\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HttpResponseError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m-> 1946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess_http_response_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\ai\\textanalytics\\_response_handlers.py:63\u001b[0m, in \u001b[0;36mprocess_http_response_error\u001b[1;34m(error)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m     62\u001b[0m     raise_error \u001b[38;5;241m=\u001b[39m ResourceNotFoundError\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m raise_error(response\u001b[38;5;241m=\u001b[39merror\u001b[38;5;241m.\u001b[39mresponse, error_format\u001b[38;5;241m=\u001b[39mCSODataV4Format) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m\n",
      "\u001b[1;31mHttpResponseError\u001b[0m: (InvalidParameterValue) Job task: 'AbstractiveSummarization-0' failed with validation errors: ['Invalid Document in request.']\nCode: InvalidParameterValue\nMessage: Job task: 'AbstractiveSummarization-0' failed with validation errors: ['Invalid Document in request.']\nException Details:\t{'code': 'InvalidRequest', 'message': \"Job task: 'AbstractiveSummarization-0' failed with validation error: Request Payload sent is too large to be processed. Limit request size to: 125000\"}"
     ]
    }
   ],
   "source": [
    "with open('./transcription.txt', 'r') as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "res_abstract = azure_abstractive_summarization(transcription)\n",
    "\n",
    "with open('./res_abstract.txt', 'w') as file:\n",
    "    file.write(res_abstract)\n",
    "\n",
    "count_words(transcription)\n",
    "count_words(res_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check document length fist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 74840, Character count: 428953\n",
      "4\n",
      "Word count: 21725, Character count: 125000\n",
      "Word count: 21839, Character count: 125000\n",
      "Word count: 21834, Character count: 125000\n",
      "Word count: 9443, Character count: 53953\n"
     ]
    }
   ],
   "source": [
    "with open('./transcription.txt', 'r') as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "count_words(transcription)\n",
    "\n",
    "def split_document(document: str, max_length: int = 125000) -> list:\n",
    "    return [document[i:i + max_length] for i in range(0, len(document), max_length)]\n",
    "\n",
    "doc = split_document(transcription)\n",
    "print(len(doc))\n",
    "\n",
    "n4 = count_words(doc[0])\n",
    "n1 = count_words(doc[1])\n",
    "n2 = count_words(doc[2])\n",
    "n3 = count_words(doc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The document appears to be a casual conversation among individuals discussing the development and investment opportunities in soften beach areas of Bali, focusing on kayaking and water-based activities. Participants highlight the continuous growth and the allure of investing in this sector, noting the island's charm and the potential for lucrative returns based on proximity to the water. They touch upon the local kayaking scene, which is vibrant and possibly undervalued or underestimated in terms of investment potential. The conversation also implies a sense of local pride and a call to recognize the ongoing development and investment opportunities that Bali offers, despite any initial uncertainties or lack of formal investment analysis.\\nStringSummary:\\nThe conversation among individuals revolves around the investment and development opportunities in Bali's beach areas, with a particular focus on kayaking as a growing sector. They discuss the island's appeal and the potential for investment returns linked to water proximity. There's an underlying tone of local pride and a call to action to acknowledge Bali's development in this niche market. The dialogue hints at a possibly informal or nascent stage of investment discussions, rich in local insights but lacking in formal financial analysis. \""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_abstractive_summarization(doc[0])\n",
    "azure_abstractive_summarization(doc[1])\n",
    "azure_abstractive_summarization(doc[2])\n",
    "azure_abstractive_summarization(doc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK\n",
    "text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(key),\n",
    "    )\n",
    "\n",
    "# OK\n",
    "def azure_abstractive_summarization(text_input: str) -> str:\n",
    "    if not isinstance(text_input, list):\n",
    "        text_input = [text_input]\n",
    "    abstract_summary_results = text_analytics_client.begin_abstract_summary(text_input).result()\n",
    "    summaries_text = \"\"\n",
    "    for result in abstract_summary_results:\n",
    "        if result.kind == \"AbstractiveSummarization\":\n",
    "            summaries_text += \"\\n\".join([summary.text for summary in result.summaries]) + \" \"\n",
    "        elif result.is_error:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                result.error.code, result.error.message\n",
    "            ))\n",
    "            return \"\"\n",
    "    return summaries_text\n",
    "\n",
    "# OK\n",
    "def azure_extractive_summarization(text_input: str) -> str:\n",
    "    if not isinstance(text_input, list):\n",
    "        text_input = [text_input]\n",
    "    extract_summary_results = text_analytics_client.begin_extract_summary(text_input).result()\n",
    "    summaries_text = \"\"\n",
    "    for result in extract_summary_results:\n",
    "        if not result.is_error:\n",
    "            summaries_text += \"\\n\".join([sentence.text for sentence in result.sentences]) + \" \"\n",
    "        else:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                result.error.code, result.error.message\n",
    "            ))\n",
    "            return \"\"\n",
    "    return summaries_text\n",
    "\n",
    "# OK\n",
    "def split_document(document: str, max_length: int = 125000) -> list:\n",
    "    return [document[i:i + max_length] for i in range(0, len(document), max_length)]\n",
    "\n",
    "# Refine Summarization\n",
    "def refine_summarization(document: str) -> str:\n",
    "    # Split the document if it exceeds the maximum length\n",
    "    documents = split_document(document)\n",
    "    # Summarize each document\n",
    "    summaries = [azure_abstractive_summarization(doc) for doc in documents]\n",
    "    # Combine the summaries\n",
    "    summaries =  \" \".join(summaries)\n",
    "    # New summary\n",
    "    new_summaries = azure_abstractive_summarization(summaries)\n",
    "    return new_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./transcription.txt', 'r') as file:\n",
    "    transcription = file.read()\n",
    "    \n",
    "res_abstract = refine_summarization(transcription)\n",
    "\n",
    "with open('./res_abstract.txt', 'w') as file:\n",
    "    file.write(res_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine summarization (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./transcription.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     70\u001b[0m     transcription \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m---> 71\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhybrid_summarization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscription\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from grapheme import length\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(key),\n",
    ")\n",
    "\n",
    "# Keep your original split function\n",
    "def split_document(document: str, max_length: int = 125000) -> list:\n",
    "    return [document[i:i + max_length] for i in range(0, len(document), max_length)]\n",
    "\n",
    "def process_chunk(chunk: str, operation: str):\n",
    "    \"\"\"Process a chunk with specified summarization type\"\"\"\n",
    "    if operation == \"abstractive\":\n",
    "        poller = text_analytics_client.begin_abstract_summary([chunk])\n",
    "    elif operation == \"extractive\":\n",
    "        poller = text_analytics_client.begin_extract_summary([chunk])\n",
    "    return poller.result()\n",
    "\n",
    "async def parallel_summarization(chunks: list, operation: str) -> list:\n",
    "    \"\"\"Process chunks in parallel\"\"\"\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_chunk, chunk, operation) for chunk in chunks]\n",
    "        return [future.result() for future in futures]\n",
    "\n",
    "def handle_results(results: list, operation: str) -> str:\n",
    "    \"\"\"Extract text from results based on operation type\"\"\"\n",
    "    summary = []\n",
    "    for result in results:\n",
    "        for doc in result:\n",
    "            if doc.is_error:\n",
    "                print(f\"Error: {doc.error.code} - {doc.error.message}\")\n",
    "                continue\n",
    "                \n",
    "            if operation == \"abstractive\":\n",
    "                if doc.kind == \"AbstractiveSummarization\":\n",
    "                    summary.extend([s.text for s in doc.summaries])\n",
    "            elif operation == \"extractive\":\n",
    "                summary.extend([s.text for s in doc.sentences])\n",
    "    \n",
    "    return \" \".join(summary)\n",
    "\n",
    "async def hybrid_summarization(document: str) -> str:\n",
    "    \"\"\"Combined abstractive and extractive summarization\"\"\"\n",
    "    # First split using your original function\n",
    "    initial_chunks = split_document(document)\n",
    "    \n",
    "    # Process both summarization types in parallel\n",
    "    abstract_results = await parallel_summarization(initial_chunks, \"abstractive\")\n",
    "    extract_results = await parallel_summarization(initial_chunks, \"extractive\")\n",
    "    \n",
    "    # Combine results from both methods\n",
    "    combined = handle_results(abstract_results, \"abstractive\") + \" \" + \\\n",
    "               handle_results(extract_results, \"extractive\")\n",
    "    \n",
    "    # Check final length and refine if needed\n",
    "    if length(combined) > 5120:\n",
    "        refined_chunks = split_document(combined)\n",
    "        final_results = await parallel_summarization(refined_chunks, \"abstractive\")\n",
    "        return handle_results(final_results, \"abstractive\")\n",
    "    \n",
    "    return combined\n",
    "\n",
    "import asyncio\n",
    "\n",
    "with open('./transcription.txt', 'r') as file:\n",
    "    transcription = file.read()\n",
    "result = asyncio.run(hybrid_summarization(transcription))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bagi banyak kasih ya malah. Jadi nomor satu masih opsi anda tetangga pak. Sorry aku boleh izin rekam ya soalnya buat buat nonton takutnya kita ada lupa aja sih sebenarnya. Saya tunggu satu masih lsy. Nomor duanya setiap tiba tiba. Kemarin sih asia timur banyak Kazakhstan sekali promonya india. Oh iya oh iya kemarin india jadi india nih lagi ramai banget di Bali. Iya sih kamu kemarin nilai biasa kita memang sama nih om ayamnya juga kan idealnya ini dia ayamnya juga kita ayamnya india ini india lagi. Amin banget datang. Kerja juga enggak nulisnya. Kita lihat data tourism nih. 3 baru china cimbo. Tempatnya. Hores selatan sama juki di situ. Jadi memang kalau bulenya ini lagi agak ini bu ya. Lagi agak menurun nampil balik lagi nanti marketnya. Sinar Mas helmy apakah memang mau main lisold atau pre old atau 2 duanya? Atau mau 2 duanya karena kalau dia. Lice hoax pasti target marketnya kan? Tanyakan beli mulai orang tua walaupun bule sekarang sudah banyak beli free hold pakai PMA. Melalui PMA dia bisa beli tripel sampai HGD. Jadi dia bikin PT oh harus sama PT mereka bikin PT boleh bolehnya bikin PT dia jadi pma karena model asing beli properti di Bali bisa sampai free hold dia lalu ada orang lokalnya di dalam. Misalkan direkturnya orang lokal, salah satu kekuatan hukum Indonesia bagian ekonomi Indonesia pt berarti ya dan dia mereka buat klik properti doang. Bisa jadi mereka memang di sini atau belum? Bisa jadi memang mereka hamil untuk. Beli properti. Karena banyak juga kan beberapa kalau yang. Yang disung enggak enaknya ya memang di lounge dipastikan banyak juga. Iya kata teman jadi kita ada teman juga kemarin ketemu katanya banyak wani launching juga di Bali sebenarnya ya makan bulan susah, kadang kadang identifikasinya ya. Terus um kalau marketnya. Kan gini pak, saya mau nanya tadi kan sebenarnya kita juga dari dari yang kita lihat. Kemarin sih kita akan rekomendasi vilanya 2 nih satu di Solo satu. Nah kalau freku kan otomatis untuk domestik. Nah sekarang untuk riskot nih resolve ini kan sebenarnya challenge challenge nya adalah kita harus bisa jualinya keluar gitu kan? Kalau yang dari kemarin kita lihat kan ada salah satunya yola tuh pak yolla itu kan dia cukup kuat ya dia dia rusia ya gitu ya kayak dia romani ya dia romania, romania, romania. Nah dia tuh cukup banyak projectnya dan bagus lah dan dia itu dari konstruksi sampai manajemen sama developer itu mereka punya semua gitu nama pinggul. Di kabel l pala. Terus apa namanya night ya? Dia bilang bahwa dia itu memang mas harinya keluar baik untuk primary maupun second tadi kalau second tadi kan untuk sewa sewanya kan gitu. Nah kalau untuk sewa oke lah gitu. Tapi untuk yang premary ini kalau kita mau this hold gitu ya challengenya begitu itu enggak sih pak? Tapi kalau. Kalau mau gini aja opsional aja. Jadi sekarang ini rata rata project itu opsional jadi gini kita jual free hold sebenarnya, tapi kan enggak menurut kemungkinan dari 10 kliennya 2 ada orang yang mereka mau beli bisa enggak kita kasih lah opsi place nah tergantung di soalnya dari kebijakannya silakan mas lain nanti mau berapa lama kan? Dulu ada cita jeansku lama banget bu ini canggih dulu citain situ 99 tahun 100 tahun lah ke dalam jadi kayak seumur hidup. Diperpanjang diperpanjang dia skemanya pasti secara administrasi perpanjang perpanjang perpanjang perpanjang 30 13. Tapi dia sudah tantangan di depan gitu loh. Sudah di depan jadi si nasabah si klien tahunya ya sudah iya saya 100 tahun itu harusnya lagi mahal yang pak harganya ya harganya dulu dia cukup tinggi, tapi dia memang enggak jualan ring karena setelah tata itu di sana susah bu hampir dikatakan hampir hampir tidak bisa hampir saya dengar ada yang bisa, tapi saya sampai sekarang belum lihat sertifikat pemerasannya masih kemarin itu juga ada yang si apa namanya yang evolit itu nama projectnya pandawa. Pondok pak kita rasanya pak kayak kalau kondom minum ini sertifikatnya apa strata title kita bilang enggak list volt dia bilang tapi emang bisa ya pak ya bisa bisa gitu contohnya yang tidak itu bisa banyak orang tamora juga gitu, tamora bilangnya. Oh pre cold tamora. Tamoreng tamu tamu tapi yang beli banyak yang orang asia memang. Tapi saya juga bingung dia strata pokoknya sampai sekarang saya belum belum lihat dia benar benar dia bisa lewat pertelan. Pokoknya kita enggak tahu ya jadi itu masih belum jelas rata rata rata kalau sudah high rise bukan hairs lah ya kalau rice takut sudah rice itu resolve. Nah kalau baru bisa sertifikat biasa. Nah kamera. Juga bilangnya ya bilang aja jualannya free call. Kalau bapak nih misalnya kita di tanah kita misalnya gitu ya bapak lebih pede mana jualin yang strata? Ini mungkin apa namanya bu enggak sih pilihan pilihan yang. Dengan prival juga prival dengan. Kondisional risol. Jadi kita ngegraph semuanya daripada kita spesifik tanah kata ya pasti kan ini kan. Karena saya atau lagi namanya teman pt ini lagi bel ya pt lgbt lgbt kalau duduknya tanah lgb. Kan bisa kan jual semua kan enggak ada yang dipegang, men'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = split_document(transcription)\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract summary:\n",
      "The document discusses the burgeoning digital nomad scene in Bali, highlighting the potential for growth in this sector due to its target market. The author expresses anticipation for policy changes that could further facilitate this lifestyle, while also noting current promising developments in Kazakhstan and India as destinations. There is a sense of opportunity in the local tourism data, with a focus on the challenge of differentiating and marketing to the ideal customer base. The conversation touches on pricing strategies, suggesting that goods like salmon and accommodations like Freeport could become more accessible. The overall tone suggests optimism for the future of digital nomadism in these regions, with a call to adapt to changing policies and market demands. The discussion implies a broader trend of global mobility and the potential for local economies to capitalize on this growing niche.\n",
      "The document appears to be a chaotic and informal discussion or transcription about various topics, likely from a social context or forum. It touches upon subjects such as a new level or national context, with a focus on Mandarin and resorts in Bandung, a mention of uang (money or currency), and a reference to a \"ferwin\" (possibly a slang term or specific context). There is also a conversation about military presence in Kalimantan, investments in underpasses, and concerns about local designs and authenticity, possibly in the tourism or local development sectors. The discussion seems to be a mix of observations, concerns, and local colloquialisms, with a sense of place and a hint of skepticism or humor about modern developments and claims. The coherence of the document is challenging due to the disjointed nature of the sentences, but it suggests a local dialogue about changes, culture, and economic activities. To fully understand the context, more background information about the location and the subjects discussed would be necessary.\n",
      "The document appears to be a casual conversation or plan among entrepreneurs discussing various business opportunities in a location that seems to be rich in potential for development, likely in a tourist-friendly area given references to full green zones, vlogging, and pariwisata (tourism). They discuss converting unused land into profitable ventures such as selling or developing atop, creating unique experiences like specialized guitar-related activities, and large-scale projects that could benefit from significant investment. There's also mention of local food and kayaking opportunities, suggesting a focus on local culture and outdoor activities. The conversation touches on the potential for growth despite current underutilization of resources and the possibility of expanding businesses beyond local confines, hinting at a vision for substantial economic development.\n",
      "Struggling to extract specific business plans or projects due to the informal and conversational nature of the text, the summary captures the entrepreneurial spirit, focus on tourism and local culture, and aspirations for large-scale development. The exact nature of the businesses or the current status of the projects remains unclear, but there's a clear intent to capitalize on the locale'ded potential.\n",
      "The document appears to be a casual conversation among individuals discussing the development and investment opportunities in soften beach areas of Bali, focusing on kayaking and water-based activities. Participants highlight the continuous growth and the allure of investing in this sector, noting the island's charm and the potential for lucrative returns based on proximity to the water. They touch upon the local kayaking scene, which is vibrant and possibly undervalued or underestimated in terms of investment potential. The conversation also implies a sense of local pride and a call to recognize the ongoing development and investment opportunities that Bali offers, despite any initial uncertainties or lack of formal investment analysis.\n",
      "StringSummary:\n",
      "The conversation among individuals revolves around the investment and development opportunities in Bali's beach areas, with a particular focus on kayaking as a growing sector. They discuss the island's appeal and the potential for investment returns linked to water proximity. There's an underlying tone of local pride and a call to action to acknowledge Bali's development in this niche market. The dialogue hints at a possibly informal or nascent stage of investment discussions, rich in local insights but lacking in formal financial analysis.\n",
      "\n",
      "Extractive summary:\n",
      "Jadi dia bikin PT oh harus sama PT mereka bikin PT boleh bolehnya bikin PT dia jadi pma karena model asing beli properti di Bali bisa sampai free hold dia lalu ada orang lokalnya di dalam.\n",
      "Jimbaran paling tinggi ya?\n",
      "kan gini pak saya mau nanya tadi kan sebenarnya kita juga dari dari yang kita lihat kemarin sih kita akan rekomendasi vilanya dua nih satu di solo satu nah kalau free kan otomatis untuk domestik nah sekarang untuk riskot nih resolve ini kan sebenarnya challenge challenge nya adalah kita harus bisa jualinya keluar gitu kan kan gini pak saya mau nanya tadi kan sebenarnya kita juga dari dari yang kita lihat kemarin sih kita akan rekomendasi vilanya dua nih satu di solo satu nah kalau freku kan otomatis untuk domestik nah sekarang untuk riskot nih resolve ini kan sebenarnya challenge challenge nya adalah kita harus bisa jualinya keluar gitu kan iya kan gini pak saya mau nanya tadi kan sebenarnya kita juga dari dari yang kita lihat kemarin sih kita akan rekomendasi vilanya dua nih satu di solo satu nah kalau free kan otomatis untuk domestik nah sekarang untuk riskot nih resolve ini kan sebenarnya challenge challenge nya adalah kita harus bisa jualinya keluar gitu kan iya kan gini pak saya mau nanya tadi kan sebenarnya kita juga dari dari yang kita lihat kemarin sih kita akan rekomendasi vilanya dua nih satu di solo satu nah kalau freeow kan otomatis untuk domestik nah sekarang untuk riskot nih resolve ini kan sebenarnya challenge challenge nya adalah kita harus bisa jualinya keluar gitu kan Kalau yang dari kemarin kita lihat kan ada salah satunya yola tuh pak yolla itu kan dia cukup kuat ya dia dia rusia ya gitu ya kayak dia romani ya dia romania, romania, romania.\n",
      "PDI juga lawannya ini di gajah dari Gerindra gerai Gerindra di gajah ini disupport sama pusat.\n",
      "oh satu sembilan tiga bali in Internasional school 3 itu paling mahal.\n",
      "terbesar di dalam ya terbesar di dunia ya terbesar terbesar di dunia Bapak bapak orang bali apa orang bukan saya orang Jakarta tapi bapak kapan kita ke balinya saya bolak balik itu mulai dari 2 18.\n",
      "berita sabtu mau jadi landmark tuh siap ini apa kalau dia punya tanah enggak dia tanah kalau dia enggak punya uang dia carikan investor kalau dia punya tanah dan punya uang ya udah dia punya uang senior investor biasanya siapa pak investornya biasanya siapa kita lihat investor beberapa yang berita sabtu mau jadi landmark tuh siap ini apa kalau dia punya tanah enggak dia tanah kalau dia punya uang dia carikan investor kalau dia punya tanah dan punya uang ya udah dia punya tanah ada uang senior investor biasanya siapa pak investornya biasanya siapa kita lihat investor beberapa yang berita sabtu mau jadi landmark tuh siapin apa kalau dia punya tanah enggak dia tanah kalau dia enggak punya uang dia carikan investor kalau dia punya tanah dan punya uang ya udah dia punya tanah ada uang senior investor biasanya siapa pak investornya biasanya siapa kita lihat investor beberapa yang berita sabtu mau jadi landmark tuh siap ini apa kalau dia punya tanah enggak dia tanah kalau dia punya uang dia carikan investor kalau dia punya tanah dan punya uang ya udah dia punya uang senior investor biasanya siapa pak investornya biasanya siapa kita lihat investor beberapa yang Dia tuh ada proyek gue mau ikut dia bisa cipin 100% atau enggak dia 50% atau enggak dia cuma dapat jatah sisanya berapa di Bali flexible tapi kebanyakan landnya lokal ke sana.\n",
      "dia tuh ada proyek gue mau ikut dia bisa cipin seratus persen atau enggak dia lima puluh persen atau enggak dia cuma dapat jatah sisanya berapa di bali flexible tapi kebanyakan lain lorch nya lokal ke sana download biakan lokal nah pembelian nah dia tuh ada proyek gue mau ikut dia bisa ciptin seratus persen atau enggak dia lima puluh persen atau enggak dia cuma dapat jatah sisanya berapa di bali flexible tapi kebanyakan landnya lokal ke sana download biakan lokal nah pembelian nah dia tuh ada proyek gue mau ikut dia bisa cipin seratus persen atau enggak dia lima puluh persen atau enggak dia cuma dapat jatah sisanya berapa di bali flexible tapi kebanyakan landnya lokal ke sana download biakan lokal nah pembelian dia tuh ada proyek gua mau ikut dia bisa cipin seratus persen atau enggak dia lima puluh persen atau enggak dia cuma dapat jatah sisanya berapa di bali flexible tapi kebanyakan landnya lokal ke sana download biakan lokal nah pembelian nah Inilah saatnya kita kalau enggak mau kerja sama misalkan kayak saya nih.\n",
      "lama dia ya hasil pelaku ini baru ngumpul nih pas dia liburan liburan bisa bulan ini satu bulan depan yang mau bulan depannya jadi lima jadi lingkat lima tuh harus di globalin enggak bisa kita pukul rata tahun ini bulan januari juga terus gini tiga segini bisa jadi nanti bulan sepuluh sebelas sebelas untuk gitu enggak tentu harus cari lihatnya yearly sih year bukan meremas lagi intinya yearly lama dia ya hasil kolam ini baru ngumpul nih pas dia liburan liburan bisa bulan ini satu bulan depan yang mau bulan depannya jadi lima jadi lingkat lima tuh harus di globalin sih enggak bisa kita pukul rata tahun ini bulan januari juga terus gini tiga segini bisa jadi nanti bulan sepuluh sebelas sebelas untuk gitu enggak tentu harus cari lihatnya yearly sih year bukan meremas lagi intinya yearly lama dia ya hasil pelaku ini baru ngumpul nih pas dia liburan liburan bisa bulan ini satu bulan depan yang mau bulan depannya jadi lima jadi minta lima tuh harus di globalin sih enggak bisa kita pukul rata tahun ini bulan januari juga terus gini tiga segini bisa jadi nanti bulan sepuluh sebelas sebelas untuk gitu enggak tentu harus cari lihatnya yearly sih year bukan meremas lagi intinya yearly lama dia ya hasil pelaku ini baru ngumpul nih pas dia liburan liburan bisa bulan ini satu bulan depan yang mau bulan depannya jadi lima jadi lingkat lima tuh harus di globalin sih enggak bisa ya kita pukul rata tahun ini bulan januari juga terus gini tiga segini bisa jadi nanti bulan sepuluh sebelas sebelas untuk gitu enggak tentu harus cari lihatnya yearly sih year bukan meremas lagi intinya yearly Itu baru fairnya kalau year berapa para tenaga paling tinggi paling rendah dan rata rata.\n",
      "minyak kita tapi kita buat apa namanya ini ya satu koma empat seribu lima ratus sebelah itu ya berapa kita lebih kita pantainya sebenarnya ini sih pak satu setengah kilo satu setengah kilo pantai tapi memang ada ada yang beberapa part bukan minyak kita tapi kita buat apa namanya ini ya satu koma empat seribu lima ratus sebelah itu ya berapa kita lagu kita pantainya sebenarnya ini sih pak satu setengah kilo satu setengah kilo pantai tapi memang ada ada yang beberapa part bukan minyak kita tapi kita buat apa namanya ini ya satu koma empat Buat apa mukanya yang ke jalan langsung johnny ya front TC kita satu 4 cuma memang di sini ada ada yang sempit menyempitan gitu ya kayaknya ini sepanjang ini dari terhubung ini agus mulai gini.\n",
      "Sewain pilot sama sama beli dominan yang satu enggak ada gaya lium dan satu view yang satu sewa 2 juta bisa juga setengah juta ya terdakwa ya karena view itu ya kalau di gitu saya wakil rakyat yang view melaporkan pasti kan pasarnya beda dulu ada project yang si pandawa residence kita dia pakai rental full, jadi dia ada gila yang kayaknya bagus sama vila yang sudah beli.\n",
      "kalau bicara market ya apa namanya harga ya kalo bicara market ya harga ya kalo bicara market ya apa namanya harga ya Mungkin saat ini.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_document(document: str, max_length: int = 125000) -> list:\n",
    "    # Split the document into chunks of max_length characters\n",
    "    return [document[i:i + max_length] for i in range(0, len(document), max_length)]\n",
    "\n",
    "def azure_abstractive_summarization(document: str) -> str:\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(key),\n",
    "    )\n",
    "\n",
    "    # Split the document if it exceeds the maximum length\n",
    "    documents = split_document(document)\n",
    "\n",
    "    summaries_text = \"\"\n",
    "    for doc in documents:\n",
    "        abstract_summary_results = text_analytics_client.begin_abstract_summary([doc]).result()\n",
    "        for result in abstract_summary_results:\n",
    "            if result.kind == \"AbstractiveSummarization\":\n",
    "                summaries_text += \"\\n\".join([summary.text for summary in result.summaries]) + \"\\n\"\n",
    "            elif result.is_error:\n",
    "                print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                    result.error.code, result.error.message\n",
    "                ))\n",
    "                return \"\"\n",
    "    \n",
    "    return summaries_text\n",
    "\n",
    "def azure_extractive_summarization(document: str) -> str:\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(key),\n",
    "    )\n",
    "\n",
    "    # Split the document if it exceeds the maximum length\n",
    "    documents = split_document(document)\n",
    "\n",
    "    summaries_text = \"\"\n",
    "    for doc in documents:\n",
    "        extract_summary_results = text_analytics_client.begin_extract_summary([doc]).result()\n",
    "        for result in extract_summary_results:\n",
    "            if not result.is_error:\n",
    "                summaries_text += \"\\n\".join([sentence.text for sentence in result.sentences]) + \"\\n\"\n",
    "            else:\n",
    "                print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                    result.error.code, result.error.message\n",
    "                ))\n",
    "                return \"\"\n",
    "    \n",
    "    return summaries_text\n",
    "\n",
    "# Example usage:\n",
    "with open('./transcription.txt', 'r') as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "abstract_summary = azure_abstractive_summarization(transcription)\n",
    "extractive_summary = azure_extractive_summarization(transcription)\n",
    "\n",
    "print(f\"Abstract summary:\\n{abstract_summary}\")\n",
    "print(f\"Extractive summary:\\n{extractive_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 1281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1281"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
