{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import streamlit as st\n",
    "import noisereduce as nr\n",
    "from scipy.io import wavfile\n",
    "import tempfile\n",
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_SPEECH_KEY, AZURE_SERVICE_REGION = \"601c0ddb54cd40709ed8efe586d8ed42\", \"southeastasia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_wav(input_file, output_file):\n",
    "    audio = AudioSegment.from_file(input_file)\n",
    "    audio.export(output_file, format=\"wav\")\n",
    "\n",
    "def reduce_noise(audio_path):\n",
    "    \"\"\"Apply noise reduction to the audio file.\"\"\"\n",
    "    if not audio_path.endswith(\".wav\"):\n",
    "        audio_path = convert_to_wav(audio_path)\n",
    "\n",
    "    rate, data = wavfile.read(audio_path)\n",
    "    reduced_noise = nr.reduce_noise(y=data, sr=rate)\n",
    "\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
    "    wavfile.write(temp_file.name, rate, reduced_noise)\n",
    "    return temp_file.name\n",
    "\n",
    "def azure_speech_to_text(audio_path):\n",
    "    \"\"\"Transcribe audio using Azure Speech Services.\"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=AZURE_SPEECH_KEY, region=AZURE_SERVICE_REGION)\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=audio_path)\n",
    "\n",
    "    # Enable auto language detection\n",
    "    auto_language_config = speechsdk.languageconfig.AutoDetectSourceLanguageConfig(languages=[\"en-US\", \"id-ID\"])\n",
    "\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "        speech_config=speech_config,\n",
    "        audio_config=audio_config,\n",
    "        auto_detect_source_language_config=auto_language_config\n",
    "    )\n",
    "\n",
    "    all_transcriptions = []\n",
    "\n",
    "    def recognized_handler(evt):\n",
    "        if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "            detected_language = evt.result.language\n",
    "            text = evt.result.text\n",
    "            all_transcriptions.append(f\"[{detected_language}] {text}\")\n",
    "\n",
    "    speech_recognizer.recognized.connect(recognized_handler)\n",
    "\n",
    "    try:\n",
    "        speech_recognizer.start_continuous_recognition()\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "    except Exception as e:\n",
    "        return f\"Error during Azure transcription: {e}\"\n",
    "\n",
    "    return \"/n\".join(all_transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tapi tu sangat.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path = \"D:/Github/Speech-to-Text-Summarization/data/audio/Natadesa/Recording 6.m4a\"\n",
    "\n",
    "convert_to_wav(audio_path, \"temp_audio.wav\")\n",
    "speech_config = speechsdk.SpeechConfig(subscription=AZURE_SPEECH_KEY, region=AZURE_SERVICE_REGION)\n",
    "audio_config = speechsdk.audio.AudioConfig(filename=\"temp_audio.wav\")\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "speech_recognition_result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak into your microphone.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError details: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(cancellation_details\u001b[38;5;241m.\u001b[39merror_details))\n\u001b[0;32m     23\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you set the speech resource key and region values?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m \u001b[43mrecognize_from_microphone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 12\u001b[0m, in \u001b[0;36mrecognize_from_microphone\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m speech_recognizer \u001b[38;5;241m=\u001b[39m speechsdk\u001b[38;5;241m.\u001b[39mSpeechRecognizer(speech_config\u001b[38;5;241m=\u001b[39mspeech_config, audio_config\u001b[38;5;241m=\u001b[39maudio_config)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeak into your microphone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m speech_recognition_result \u001b[38;5;241m=\u001b[39m \u001b[43mspeech_recognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize_once_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m speech_recognition_result\u001b[38;5;241m.\u001b[39mreason \u001b[38;5;241m==\u001b[39m speechsdk\u001b[38;5;241m.\u001b[39mResultReason\u001b[38;5;241m.\u001b[39mRecognizedSpeech:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecognized: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(speech_recognition_result\u001b[38;5;241m.\u001b[39mtext))\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\cognitiveservices\\speech\\speech.py:575\u001b[0m, in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;124;03m    Waits until the result is available, and returns it.\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__resolved:\n\u001b[0;32m    576\u001b[0m         result_handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__wrapped_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\cognitiveservices\\speech\\speech.py:1060\u001b[0m, in \u001b[0;36mresolve_future\u001b[1;34m(handle)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve_future\u001b[39m(handle: _spx_handle):\n\u001b[1;32m-> 1060\u001b[0m     result_handle \u001b[38;5;241m=\u001b[39m _spx_handle(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1061\u001b[0m     _call_hr_fn(fn\u001b[38;5;241m=\u001b[39m_sdk_lib\u001b[38;5;241m.\u001b[39mrecognizer_recognize_once_async_wait_for, \u001b[38;5;241m*\u001b[39m[handle, max_uint32, ctypes\u001b[38;5;241m.\u001b[39mbyref(result_handle)])\n\u001b[0;32m   1062\u001b[0m     _sdk_lib\u001b[38;5;241m.\u001b[39mrecognizer_async_handle_release(handle)\n",
      "File \u001b[1;32mc:\\Users\\firmansyah.atmojo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\azure\\cognitiveservices\\speech\\interop.py:61\u001b[0m, in \u001b[0;36m_call_hr_fn\u001b[1;34m(fn, *args)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_hr_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, fn):\n\u001b[0;32m     60\u001b[0m     fn\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m _spx_hr\n\u001b[1;32m---> 61\u001b[0m     hr \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m fn()\n\u001b[0;32m     62\u001b[0m     _raise_if_failed(hr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def recognize_from_microphone():\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    #speech_config = speechsdk.SpeechConfig(subscription=os.environ.get('SPEECH_KEY'), region=os.environ.get('SPEECH_REGION'))\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=AZURE_SPEECH_KEY, region=AZURE_SERVICE_REGION)\n",
    "    speech_config.speech_recognition_language=\"en-US\"\n",
    "\n",
    "    #audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=\"temp_audio.wav\")\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")\n",
    "\n",
    "recognize_from_microphone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the audio file for continuous recognition...\n",
      "Recognized: Tetapi juga dan kawasan hukum.\n",
      "Recognized: Terus kita juga ada kerja sama sama uh biasanya harus narik manajemen hilang.\n",
      "Recognized: Buat bantu ngelola jadi harus bekerja sama dengan manajemen film kayak kita kan ngambilnya elf hva.\n",
      "Recognized: Jadi kita boleh dishare enggak sih pak? Kalau materi ini ini materi prestasi ppt paling nanti ih brosurnya aja ya yang yang petah betah tadi boleh nih ada di.\n",
      "Recognized: Di sini jujur anaknya rebel ini sih.\n",
      "Recognized: 3 episode enggak ada ya? Di website enggak ada enggak ada.\n",
      "Recognized: Makanya kalau enggak datang ke sini enggak tahu. Iya, makanya iya kalau enggak ketemu mbak lisa juga enggak tahu. Enggak tahu sih.\n",
      "Recognized: Oh ini estimate troy nya ini memang di state ya wow sampai 6 bisa di Bali.\n",
      "Recognized: Jujur lebih jauh daripada.\n",
      "Recognized: Oh tipe dia lebih cepat. Itu kan kayak sewa tahu.\n",
      "Recognized: Lebih lebih gede.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def recognize_from_audio_file_continuous():\n",
    "    #AZURE_SPEECH_KEY = \"your_speech_service_key\"\n",
    "    #AZURE_SERVICE_REGION = \"your_service_region\"\n",
    "\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=AZURE_SPEECH_KEY, region=AZURE_SERVICE_REGION)\n",
    "    speech_config.speech_recognition_language = \"id-ID\"\n",
    "\n",
    "    audio_file_path = \"temp_audio.wav\"\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=audio_file_path)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Processing the audio file for continuous recognition...\")\n",
    "\n",
    "    def recognized_callback(evt):\n",
    "        print(\"Recognized: {}\".format(evt.result.text))\n",
    "\n",
    "    done = False\n",
    "\n",
    "    def stop_cb(evt):\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    # Connect callbacks\n",
    "    speech_recognizer.recognized.connect(recognized_callback)\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # Start continuous recognition\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        pass\n",
    "    speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "recognize_from_audio_file_continuous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the audio file for transcription with timestamps...\n",
      "[00:02] Tetapi juga dan kawasan hukum.\n",
      "[00:05] Terus kita juga ada kerja sama sama uh biasanya harus narik manajemen hilang.\n",
      "[00:12] Buat bantu ngelola jadi harus bekerja sama dengan manajemen film kayak kita kan ngambilnya elf hva.\n",
      "[00:24] Jadi kita boleh dishare enggak sih pak? Kalau materi ini ini materi prestasi ppt paling nanti ih brosurnya aja ya yang yang petah betah tadi boleh nih ada di.\n",
      "[00:36] Di sini jujur anaknya rebel ini sih.\n",
      "[00:40] 3 episode enggak ada ya? Di website enggak ada enggak ada.\n",
      "[00:45] Makanya kalau enggak datang ke sini enggak tahu. Iya, makanya iya kalau enggak ketemu mbak lisa juga enggak tahu. Enggak tahu sih.\n",
      "[00:52] Oh ini estimate troy nya ini memang di state ya wow sampai 6 bisa di Bali.\n",
      "[00:59] Jujur lebih jauh daripada.\n",
      "[01:03] Oh tipe dia lebih cepat. Itu kan kayak sewa tahu.\n",
      "[01:06] Lebih lebih gede.\n"
     ]
    }
   ],
   "source": [
    "def recognize_with_segment_timestamps():\n",
    "    #AZURE_SPEECH_KEY = \"your_speech_service_key\"\n",
    "    #AZURE_SERVICE_REGION = \"your_service_region\"\n",
    "\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=AZURE_SPEECH_KEY, region=AZURE_SERVICE_REGION)\n",
    "    speech_config.speech_recognition_language = \"id-ID\"\n",
    "\n",
    "    # Request detailed output\n",
    "    speech_config.output_format = speechsdk.OutputFormat.Detailed\n",
    "\n",
    "    audio_file_path = \"temp_audio.wav\"\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=audio_file_path)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Processing the audio file for transcription with timestamps...\")\n",
    "\n",
    "    def recognized_callback(evt):\n",
    "        # Access the detailed recognition result\n",
    "        result_json = evt.result.json\n",
    "        result_dict = eval(result_json)  # Convert JSON string to a dictionary\n",
    "\n",
    "        recognized_text = result_dict[\"DisplayText\"]\n",
    "        offset = result_dict[\"Offset\"]  # Start time in 100-nanoseconds\n",
    "        duration = result_dict[\"Duration\"]  # Duration in 100-nanoseconds\n",
    "\n",
    "        # Convert offset to MM:SS format\n",
    "        start_seconds = offset / 10**7\n",
    "        start_minutes = int(start_seconds // 60)\n",
    "        start_seconds = int(start_seconds % 60)\n",
    "\n",
    "        # Print timestamp and text\n",
    "        print(f\"[{start_minutes:02}:{start_seconds:02}] {recognized_text}\")\n",
    "\n",
    "    done = False\n",
    "\n",
    "    def stop_cb(evt):\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    # Connect callbacks\n",
    "    speech_recognizer.recognized.connect(recognized_callback)\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # Start continuous recognition\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        pass\n",
    "    speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "recognize_with_segment_timestamps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
